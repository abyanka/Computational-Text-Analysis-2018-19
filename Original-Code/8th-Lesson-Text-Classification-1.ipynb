{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the small file\n",
    "# play around with the big file to see if you get an improvement in performance!\n",
    "\n",
    "import gensim, logging\n",
    "# the model is organized like this: word = embeddings\n",
    "small_model = gensim.models.KeyedVectors.load_word2vec_format('../small-embeddings.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# homeworks!\n",
    "\n",
    "import codecs, nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def text_embedding(text):\n",
    "    \n",
    "    #it depends if the words have been lowercased or not\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "        \n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    article_embedd = []\n",
    "    \n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = small_model[word]\n",
    "                article_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)]\n",
    "    return avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.26903926208615303, 0.7144050151109695, 0.6478350050747395, 0.3210650235414505, 0.07041849289089441, -0.021145001519471407, -0.7754549980163574, -0.2608862593770027, -0.2335975021123886, -0.44401000440120697, -0.7981760036200285, -0.2174225002527237, -0.2587737590074539, 0.020651994738727808, 0.16106024757027626, 0.11651949770748615, -0.40395849477499723, -0.11410426755901426, 0.19817999750375748, -0.18172174505889416, -0.09501974750310183, 0.13451825641095638, 0.24662524834275246, -0.6156899929046631, 0.17309998720884323, -2.1071474701166153, 0.201797503978014, 0.0012424960732460022, -0.7112424969673157, -0.06578348483890295, 1.8628499507904053, 0.3978800028562546, -1.0788562297821045, -0.5919300131499767, -0.5450749918818474, -1.0065224766731262, -0.4167025089263916, 0.18753925105556846, -0.6404455121737556, -0.8506049998104572, -0.3560800105333328, 0.3043750002980232, -0.3289024978876114, -0.748709999024868, 0.187304999679327, 0.5087025091052055, -0.7147175222635269, 0.20657499879598618, -0.3577454970218241, 0.6135782618075609]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Barack Obama was president of the USA\"\n",
    "\n",
    "embed_sentence = text_embedding(sentence)\n",
    "print (embed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\",\"Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with a tire they put on. They \\\"\"fixed\\\"\" it for free, and the very next morning I had the same issue. I called to complain, and the \\\"\"manager\\\"\" didn't even apologize!!! So frustrated. Never going back.  They seem overpriced, too.\"\n"
     ]
    }
   ],
   "source": [
    "# YELP product reviews dataset\n",
    "\n",
    "import codecs\n",
    "\n",
    "sentiment_dataset = codecs.open(\"yelp_review_polarity_csv/test.csv\",\"r\",\"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "print (sentiment_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.0\n",
      "3.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# AFINN Dictionary for Sentiment Analysis: https://github.com/fnielsen/afinn\n",
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "print (afinn.score(\"This is bad fake news\"))\n",
    "\n",
    "print (afinn.score(\"An exam on the 20th of December? Oh, that's great!\"))\n",
    "\n",
    "print (afinn.score(\"That movie is horrible and beautiful at the same time\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need an NLP pipeline for Sentiment Analysis\n",
    "\n",
    "import codecs, nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\")if \"V\" in pos else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = \" \".join([token for token in text if token not in stop_word_list])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, we define two folders, \"corpus\" - with the text and \"labels\", with the labels\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be careful with this, the dataset is huge!\n",
    "#for line in sentiment_dataset:\n",
    "for line in sentiment_dataset[:1000]:\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    \n",
    "    text = nlp_pipeline(text)\n",
    "    \n",
    "    corpus.append(text)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "\n",
    "for review in corpus:\n",
    "    score = afinn.score(review)\n",
    "    \n",
    "    if score < 0.0:\n",
    "        pred.append(\"-1\")\n",
    "    else:\n",
    "        pred.append(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.70287869412769965, 0.61031096300805843, 0.55213903743315518, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "print (precision_recall_fscore_support(labels, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework! Change the text processing pipeline (e.g., remove the POS tagger, keep stopwords, etc) and see if you can improve the performance of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready!\n"
     ]
    }
   ],
   "source": [
    "# first, we define two folders, \"corpus\" - with the text and \"labels\", with the labels\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be careful with this, the dataset is huge!\n",
    "#for line in sentiment_dataset:\n",
    "#for line in sentiment_dataset[:10000]: <-- by adding more training data performance will improve (i hope!)\n",
    "# however, it'll use lots of memory ;-)\n",
    "for line in sentiment_dataset[:20000]:\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    \n",
    "    text = text_embedding(text)\n",
    "    if len(text)>0:\n",
    "        corpus.append(text)\n",
    "        labels.append(label)\n",
    "print (\"ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(corpus)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.64175919097218714, 0.64182366734722751, 0.64088071654080614, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/federiconanni/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.65045851638640606, 0.65057561301016797, 0.65049716090272214, None)\n",
      "(0.66465719131066781, 0.66486688633285485, 0.66468795078173581, None)\n",
      "(0.62578580812861118, 0.62599405688410925, 0.6256495883595381, None)\n",
      "(0.65175340336855281, 0.65199114597020358, 0.65171405052679721, None)\n",
      "(0.66429393184042951, 0.66454799975742374, 0.66426991932308121, None)\n",
      "(0.64304457953394123, 0.64283835394693578, 0.64290477907911137, None)\n",
      "(0.64990841652165954, 0.64919788533406253, 0.64930166290308222, None)\n",
      "(0.66486592270302747, 0.66509433962264142, 0.66488315331232584, None)\n",
      "(0.68337613751263904, 0.68367514356029524, 0.68315981001776027, None)\n",
      " \n",
      "0.653794879175\n"
     ]
    }
   ],
   "source": [
    "#here's the documentation: http://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "final_f1 = []\n",
    "\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    final_C = 1\n",
    "    classifier = GaussianNB().fit(X_train , y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
