{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor checking the homeworks we need a few things:\\n\\n- import embeddings\\n- load our functions (text_embeddings and nlp_pipeline)\\n- load the aFinn dictionary\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for checking the homeworks we need a few things:\n",
    "\n",
    "- import embeddings\n",
    "- load our functions (text_embeddings and nlp_pipeline)\n",
    "- load the aFinn dictionary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the small file\n",
    "\n",
    "import gensim\n",
    "\n",
    "small_model = gensim.models.KeyedVectors.load_word2vec_format('../small-embeddings.txt', binary=False)\n",
    "\n",
    "#large model\n",
    "#small_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/federiconanni/WordEmbeddings/GoogleNews-vectors-negative300.bin.gz', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs, nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string, you convert text in a doc-embedding\n",
    "def text_embedding(text):\n",
    "    \n",
    "    #it depends if the words are lowercased or not in the word embeddings that you use, if they are not skip this step\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove numbers\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "\n",
    "    # remove stopwords (not essential)\n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    article_embedd = []\n",
    "    \n",
    "    # you take all embeddings\n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = small_model[word]\n",
    "                article_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    \n",
    "    # then you average them\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)]\n",
    "    \n",
    "    return avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\",\"Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with a tire they put on. They \\\"\"fixed\\\"\" it for free, and the very next morning I had the same issue. I called to complain, and the \\\"\"manager\\\"\" didn't even apologize!!! So frustrated. Never going back.  They seem overpriced, too.\"\n"
     ]
    }
   ],
   "source": [
    "# YELP product reviews dataset\n",
    "\n",
    "\n",
    "# we are using only the \"small\" test-set, you can also train on the large training set if you'd like\n",
    "sentiment_dataset = codecs.open(\"yelp_review_polarity_csv/test.csv\",\"r\",\"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "print (sentiment_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFINN Dictionary for Sentiment Analysis: https://github.com/fnielsen/afinn\n",
    "\n",
    "from afinn import Afinn\n",
    "\n",
    "afinn = Afinn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need an NLP pipeline for Sentiment Analysis\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\")if \"V\" in pos else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = \" \".join([token for token in text if token not in stop_word_list])\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, we define two lists, \"corpus\" - with the text and \"labels\", with the labels\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be careful with this, the dataset is huge (and this is only a small part of it - you can also use \"train\")!\n",
    "\n",
    "#for line in sentiment_dataset:\n",
    "for line in sentiment_dataset[:1000]:\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    \n",
    "    text = nlp_pipeline(text)\n",
    "    \n",
    "    corpus.append(text)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "\n",
    "for review in corpus:\n",
    "    score = afinn.score(review)\n",
    "    \n",
    "    if score < 0.0:\n",
    "        pred.append(\"-1\")\n",
    "    else:\n",
    "        pred.append(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.70287869412769965, 0.61031096300805843, 0.55213903743315518, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "print (precision_recall_fscore_support(labels, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework! Change the text processing pipeline (e.g., remove the POS tagger, keep stopwords, etc) and see if you can improve the performance of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# solution n.1: assign neutral to negative ;-)\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be careful with this, the dataset is huge!\n",
    "#for line in sentiment_dataset:\n",
    "for line in sentiment_dataset[:1000]:\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    \n",
    "    text = nlp_pipeline(text)\n",
    "    \n",
    "    corpus.append(text)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.67040052665802363, 0.66898300197055383, 0.66667967917168403, None)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "\n",
    "for review in corpus:\n",
    "    score = afinn.score(review)\n",
    "    # assign neutral to negative!\n",
    "    if score < 0.1:\n",
    "        pred.append(\"-1\")\n",
    "    else:\n",
    "        pred.append(\"1\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "print (precision_recall_fscore_support(labels, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# solution n.2: no text processing at all!\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be careful with this, the dataset is huge!\n",
    "#for line in sentiment_dataset:\n",
    "for line in sentiment_dataset[:1000]:\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    \n",
    "#    text = nlp_pipeline(text)\n",
    "    \n",
    "    corpus.append(text)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.66953967642526968, 0.66922731860491202, 0.66796679667966785, None)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "\n",
    "for review in corpus:\n",
    "    score = afinn.score(review)\n",
    "    # assign neutral to negative!\n",
    "    if score < 0.1:\n",
    "        pred.append(\"-1\")\n",
    "    else:\n",
    "        pred.append(\"1\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "print (precision_recall_fscore_support(labels, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready!\n"
     ]
    }
   ],
   "source": [
    "# let's train a naive bayes classifier for doing the same\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be careful with this, the dataset is huge!\n",
    "#for line in sentiment_dataset:\n",
    "\n",
    "#for line in sentiment_dataset[:10000]: <-- by adding more training data performance will improve (i hope!)\n",
    "# however, it'll use lots of memory ;-)\n",
    "for line in sentiment_dataset[:10000]:\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    \n",
    "    text = text_embedding(text)\n",
    "    if len(text)>0:\n",
    "        corpus.append(text)\n",
    "        labels.append(label)\n",
    "print (\"ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numpy array is a different type of list: https://stackoverflow.com/questions/993984/why-numpy-instead-of-python-lists\n",
    "# they are super efficient for storing information in compat ways\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# X and y are standard abbreviations for corpus and labels\n",
    "X = np.array(corpus)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.65815837581438807, 0.6589108710604038, 0.65815120491161827, None)\n",
      "(0.66081263744023233, 0.66154725424388339, 0.66082567463104336, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/federiconanni/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.64388007168574513, 0.64448659954277931, 0.64242610042597126, None)\n",
      "(0.65209825542854372, 0.65285326638881447, 0.65101799263960447, None)\n",
      "(0.65538120036874703, 0.6561070624842591, 0.65408652485709884, None)\n",
      "(0.62538521268593339, 0.62594140729727754, 0.62395309544212518, None)\n",
      "(0.64612912685418245, 0.64574447342123864, 0.64589451164455447, None)\n",
      "(0.65566198278796128, 0.65606034755904363, 0.65323130540192442, None)\n",
      "(0.65478076482488001, 0.65478076482488001, 0.65125628140703506, None)\n",
      "(0.65245495276915122, 0.65314980460974759, 0.65108570539270239, None)\n",
      " \n",
      "0.649192839675\n"
     ]
    }
   ],
   "source": [
    "#here's the documentation: http://scikit-learn.org/stable/supervised_learning.html#supervised-learning\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "final_f1 = []\n",
    "\n",
    "# cross validation 10 folds\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    classifier = GaussianNB().fit(X_train , y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready!\n"
     ]
    }
   ],
   "source": [
    "# let's compare it with a tf-idf bag-of-word approach\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be careful with this, the dataset is huge!\n",
    "for line in sentiment_dataset[:100]:\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    \n",
    "    text = nlp_pipeline(text)\n",
    "    if len(text)>0:\n",
    "        corpus.append(text)\n",
    "        labels.append(label)\n",
    "print (\"ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf, done!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l2')\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).A\n",
    "\n",
    "X = np.array(tfidf_matrix)\n",
    "y = np.array(labels)\n",
    "\n",
    "print (\"tf-idf, done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.1225629   0.          0.1225629   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.1225629\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.1225629\n",
      "  0.          0.1225629   0.1225629   0.11246635  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.1225629   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.1225629   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.1225629   0.          0.          0.          0.\n",
      "  0.          0.11246635  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.21060545  0.2451258   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.08248601  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.09520617  0.          0.09974619  0.1225629   0.          0.          0.\n",
      "  0.1225629   0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.2451258   0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print (X[50][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_f1 = []\n",
    "\n",
    "# cross validation 10 folds\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    final_C = 1\n",
    "    classifier = GaussianNB().fit(X_train , y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train an svm using word embeddings\n",
    "from sklearn import svm\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be super careful here \n",
    "for line in sentiment_dataset[:5000]:\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    \n",
    "    text = text_embedding(text)\n",
    "    if len(text)>0:\n",
    "        corpus.append(text)\n",
    "        labels.append(label)\n",
    "print (\"ready!\")\n",
    "\n",
    "X = np.array(corpus)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the role of the parameter C: https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n",
    "\n",
    "final_f1 = []\n",
    "\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    classifier = svm.SVC(kernel = \"linear\", C=10).fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train an svm using tfidf\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be super careful here - import no more than 5000 for tfidf\n",
    "for line in sentiment_dataset[:5000]:\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    \n",
    "    text = nlp_pipeline(text)\n",
    "    if len(text)>0:\n",
    "        corpus.append(text)\n",
    "        labels.append(label)\n",
    "\n",
    "print (\"ready!\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(norm='l2')\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).A\n",
    "\n",
    "X = np.array(tfidf_matrix)\n",
    "y = np.array(labels)\n",
    "\n",
    "print (\"tf-idf, done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_f1 = []\n",
    "\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    classifier = svm.SVC(kernel = \"linear\", C=1).fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework: read online what is a Nearest Centroid Classifier and implement one with word-embeddings and tf-idf features. Hint - to implement it you just need to change a couple of lines from the previous classifiers.\n",
    "\n",
    "In addition, if you want you can also check what a K-nearest Neighbors Classifier is. To implement it, you only have 1 parameter to tune - which one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
