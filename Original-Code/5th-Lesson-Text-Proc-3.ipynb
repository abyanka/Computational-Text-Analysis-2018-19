{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# homework from lesson 4\n",
    "\n",
    "import codecs, nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "dataset = codecs.open(\"dataset.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "article = dataset[50].split(\"\\t\")[3]\n",
    "\n",
    "# split into sentences\n",
    "sentences = nltk.sent_tokenize(article) \n",
    "\n",
    "sentence = nltk.word_tokenize(sentences[4])\n",
    "\n",
    "# you use the pos-tagger (it gives you back a list of tuples (word,pos))\n",
    "pos_sentence = nltk.pos_tag(sentence)\n",
    "\n",
    "lemma_word = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\") if \"V\" in pos else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in pos_sentence]\n",
    "\n",
    "print (lemma_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of writing a series of commands, like we did so far, we can write them in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's define a function that does all we need\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\")if \"V\" in pos else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's take a new article\n",
    "article = dataset[261].split(\"\\t\")[3]\n",
    "\n",
    "print (article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's use our pipeline!\n",
    "clean_article = nlp_pipeline(article)\n",
    "print (clean_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word sense disambiguation\n",
    "\n",
    "# check documentation: http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# let's isolate each word - you do this using a set (another type of object in python)\n",
    "\n",
    "unique_words = set(clean_article)\n",
    "\n",
    "# let's check how many senses each word has\n",
    "for word in unique_words:\n",
    "    print (word, len(wn.synsets(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = \"cell\"\n",
    "\n",
    "senses = wn.synsets(word)\n",
    "\n",
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    print(word, sense.definition())\n",
    "    \n",
    "    # get a textual example\n",
    "    print(sense.examples())\n",
    "    \n",
    "    # get hypernymy\n",
    "    print(sense.hypernyms())\n",
    "\n",
    "    # get hyponyms\n",
    "    print(sense.hyponyms())\n",
    "        \n",
    "    # this is a way of getting synonyms - there are others\n",
    "    print (sense.lemma_names())\n",
    "    \n",
    "    # this is for getting antonyms - works especially with adjectives \n",
    "    print (sense.lemmas()[0].antonyms())\n",
    "    \n",
    "    print (\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finding the best sense\n",
    "# let's consider two sentences where \"cell\" is mentioned\n",
    "\n",
    "sent1 = \"The terrorist cell was neutralized near the southern Russian city of Makhachkala, the capital of the Republic of Dagestan.\"\n",
    "\n",
    "sent2 = \"The molecule, which uses light energy to move protons across a somatic cell membrane, proved unsuitable for crystallography.\"\n",
    "\n",
    "# you clean the sentences using our pipeline\n",
    "clean_sent1 = nlp_pipeline(sent1)\n",
    "\n",
    "clean_sent2 = nlp_pipeline(sent2)\n",
    "\n",
    "print (\"clean sent 1:\", clean_sent1)\n",
    "print (\"clean sent 2:\", clean_sent2)\n",
    "print (\" \")\n",
    "\n",
    "# for each possible sense of \"cell\" you check the overlap between the definition and the sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    definition =  sense.definition()\n",
    "    \n",
    "    # you clean the definition with our pipeline\n",
    "    clean_definition = nlp_pipeline(definition)\n",
    "    \n",
    "    # you check the intersection of the two sentences\n",
    "    inters_1 = set(clean_sent1).intersection(clean_definition)\n",
    "    inters_2 = set(clean_sent2).intersection(clean_definition)\n",
    "    \n",
    "    print (definition)\n",
    "    print (\"clean definition:\", clean_definition)\n",
    "    print (\"intersection with sent 1:\", inters_1)\n",
    "    print (\"intersection with sent 2:\", inters_2)\n",
    "    print (len(inters_1),len(inters_2))\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sense in senses[1:]:\n",
    "    # get definition of sense\n",
    "    definition = sense.definition()    \n",
    "    print (definition)\n",
    "    # take all hypernyms, hyponyms and synonyms - you need to do a bit of cleaning\n",
    "    hypernyms = [hyper.lemmas()[0].name().replace(\"_\",\" \") for hyper in sense.hypernyms()] \n",
    "    print (hypernyms)\n",
    "    \n",
    "    hypernyms = \" \".join(hypernyms)\n",
    "    \n",
    "    print (definition+\" \" +hypernyms)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sense in senses:\n",
    "    # get definition of sense\n",
    "    definition = sense.definition()    \n",
    "    \n",
    "    # take all hypernyms, hyponyms and synonyms - you need to do a bit of cleaning\n",
    "    hypernyms = [hyper.lemmas()[0].name().replace(\"_\",\" \") for hyper in sense.hypernyms()] \n",
    "    \n",
    "    hyponyms = [hypon.lemmas()[0].name().replace(\"_\",\" \") for hypon in sense.hyponyms()] \n",
    "        \n",
    "    synonyms = [synon.replace(\"_\",\" \") for synon in sense.lemma_names()] \n",
    "    \n",
    "    # you concatenate all of them - check out online how to use \"join\" to connect elements of a list\n",
    "    sense_text = sense.definition() + \" \".join(sense.examples())  + \" \".join(hypernyms)+ \" \".join(hyponyms)+ \" \".join(synonyms)\n",
    "\n",
    "    # now you have a very long definition, which uses all these pieces of information\n",
    "    clean_definition = nlp_pipeline(sense_text)\n",
    "    \n",
    "\n",
    "    inters_1 = set(clean_sent1).intersection(clean_definition)\n",
    "    inters_2 = set(clean_sent2).intersection(clean_definition)    \n",
    "    \n",
    "    print (\"clean definition:\", clean_definition)\n",
    "    print (\"intersection with sent 1:\", inters_1)\n",
    "    print (\"intersection with sent 2:\", inters_2)\n",
    "    print (len(inters_1),len(inters_2))\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "homework: scrape recent tweets from Donald Trump and improve his vocabulary by changing his poor choice of adjectives  with more sophisticated synonyms (e.g. \"bad ratings on the Emmys last night\" -> \"substandard ratings on the Emmys last night\") \n",
    " \n",
    "or\n",
    "\n",
    "make his tweets nicer by changing adjectives with related antonyms (e.g. \"bad ratings on the Emmys last night\" -> \"excellent ratings on the Emmys last night\") \n",
    "\n",
    "to do you need to combine:\n",
    "\n",
    "- the Twitter API\n",
    "- text processing (POS tagging + WordNet)\n",
    "- and to find a solution for knowing if a word is \"more sophisticated\" than another one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
