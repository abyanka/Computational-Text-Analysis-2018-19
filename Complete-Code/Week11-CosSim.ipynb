{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homeworks! \n",
    "\n",
    "import gensim\n",
    "\n",
    "small_model = gensim.models.KeyedVectors.load_word2vec_format('../small-embeddings.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs, nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string\n",
    "def text_embedding(text):\n",
    "    \n",
    "    #it depends if the words have been lowercased or not\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "        \n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    article_embedd = []\n",
    "    \n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = small_model[word]\n",
    "                article_embedd.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    avg = [float(sum(col))/len(col) for col in zip(*article_embedd)]\n",
    "    \n",
    "    # the output is a doc-embedding\n",
    "    return avg\n",
    "\n",
    "\n",
    "# input should be a string\n",
    "def nlp_pipeline(text):\n",
    "    \n",
    "    # if you want you can split in sentences - i'm usually skipping this step\n",
    "    # text = nltk.sent_tokenize(text) \n",
    "    \n",
    "    #tokenize words for each sentence\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # pos tagger\n",
    "    text = nltk.pos_tag(text)\n",
    "\n",
    "    # lemmatizer\n",
    "    text = [wordnet_lemmatizer.lemmatize(token.lower(),\"v\")if \"V\" in pos else wordnet_lemmatizer.lemmatize(token.lower()) for token,pos in text]\n",
    "    \n",
    "    # remove punctuation and numbers\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    # remove stopwords - be careful with this step    \n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # the output is text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YELP product reviews dataset\n",
    "\n",
    "import codecs\n",
    "\n",
    "sentiment_dataset = codecs.open(\"yelp_review_polarity_csv/test.csv\",\"r\",\"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "print (sentiment_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "# be super careful here \n",
    "for line in sentiment_dataset[:5000]:\n",
    "    # we clean a bit the label (they used 1 for negative and 2 for positive, we change that in -1 and 1)\n",
    "    label = line.split(\",\")[0].replace('\"','').replace(\"1\",\"-1\").replace(\"2\",\"1\")\n",
    "    \n",
    "    # we clean a bit the text, removing quotation marks at the beginning\n",
    "    text = line.split(\",\")[1].replace('\"','')\n",
    "    \n",
    "    # we embed the text\n",
    "    text = text_embedding(text)\n",
    "    # if we have a doc-embedding, then we save doc-embedding and label\n",
    "    if len(text)>0:\n",
    "        corpus.append(text)\n",
    "        labels.append(label)\n",
    "print (\"ready!\")\n",
    "\n",
    "# we use np array as they are more efficient\n",
    "X = np.array(corpus)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework 1 - knn classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "final_f1 = []\n",
    "# we set that we do 10 fold cross validation\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "# for each of the 10 round\n",
    "for train, test in kf_total:\n",
    "    # we define training and test embeddings and labels\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    # we train the classifier (using 50 neighbors, but you can change that)\n",
    "    # we train on the training set, using embeddings and labels\n",
    "    classifier = KNeighborsClassifier(n_neighbors=50).fit(X_train, y_train) \n",
    "    \n",
    "    # then we test it on the test set, we provide the embeddings and we make the classifier predict the labels\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # then we compare the prediction with the true test-labels using precision, recall and f1 (ignore the last None column)\n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "    \n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homework 2 - nearest centroid\n",
    "\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "final_f1 = []\n",
    "\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "for train, test in kf_total:\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    classifier = NearestCentroid().fit(X_train, y_train) \n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print (precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    f1_score = precision_recall_fscore_support(y_test, y_pred, average=\"macro\")[2]\n",
    "    final_f1.append(f1_score)\n",
    "print (\" \")\n",
    "print (sum(final_f1)/len(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's classify the articles from rt.com in topics!\n",
    "# we can compare the performances of different classifiers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "\n",
    "dataset = codecs.open(\"dataset.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "article = dataset[4].split(\"\\t\")\n",
    "\n",
    "corpus = []\n",
    "labels =  []\n",
    "\n",
    "# we load the first 5000 lines of our dataset (you can try to load it all at home)\n",
    "for line in dataset[1:5000]:\n",
    "    # the topic, like \"usa\" \"uk\", etc is the label that we want to predict\n",
    "    label = line.split(\"\\t\")[2]\n",
    "    text = line.split(\"\\t\")[3]\n",
    "    # as usual, we use embeddings\n",
    "    text = text_embedding(text)\n",
    "    \n",
    "    if len(text)>0:\n",
    "        corpus.append(text)\n",
    "        labels.append(label)\n",
    "print (\"ready!\")\n",
    "\n",
    "# again, we use np arrays as they are more efficient\n",
    "X = np.array(corpus)\n",
    "y = np.array(labels) \n",
    "\n",
    "# we set here 4 different types of classifier\n",
    "kNN =  KNeighborsClassifier(n_neighbors=5)\n",
    "NearCentroid = NearestCentroid()\n",
    "naiveBayes = GaussianNB()\n",
    "SVM = svm.SVC(kernel = \"linear\", C=1) \n",
    "\n",
    "final_f1 = []\n",
    "# then in the ususal 10 fold cross validation setting\n",
    "kf_total = cross_validation.StratifiedKFold(y, n_folds=10, shuffle=True)\n",
    "for train, test in kf_total:\n",
    "    \n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "    \n",
    "    # we test them all, one after the other\n",
    "    classifier = kNN.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print (\"kNN\", precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "    classifier = NearCentroid.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print (\"NearCentroid\", precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "    \n",
    "    classifier = naiveBayes.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print (\"NaiveBayes\", precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "    classifier = SVM.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print (\"SVM\", precision_recall_fscore_support(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "    \n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 1: SVM multi-class is by standard one-vs-one. Learn the difference between SVM one-vs-one and one-vs-rest. Implement a one-vs-rest svm classifier (hint: it's not suuper obvious, but google a bit around).\n",
    "\n",
    "Homework 2: run the same experiments, but use tf-idf features instead of word embeddings (be careful with the number of docs that you use, start with something link 100/200 examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function that I copy-pasted from here: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "# these are visualizations to study the type of error that the classifier is making\n",
    "# we use here the last y_test and y_pred in memory, so the last svm run\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# this function creates a confusion matrix, is only to show you a way of doing some testing\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classifier.classes_))\n",
    "    plt.xticks(tick_marks, classifier.classes_, rotation=90)\n",
    "    plt.yticks(tick_marks, classifier.classes_)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix (using the scores from the SVM from before)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to cluster articles\n",
    "\n",
    "# i'm re-loading everything here, because I want to use the titles of the articles to interpret the clusters\n",
    "dataset = codecs.open(\"dataset.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "article = dataset[4].split(\"\\t\")\n",
    "\n",
    "corpus = []\n",
    "titles =  []\n",
    "\n",
    "# you can run wit all data at home\n",
    "for line in dataset[1:1000]:\n",
    "    # to better understands which clusters are created, let's check the titles of the articles\n",
    "    title = line.split(\"\\t\")[1]\n",
    "    text = line.split(\"\\t\")[3]\n",
    "    text = text_embedding(text)\n",
    "    \n",
    "    if len(text)>0:\n",
    "        corpus.append(text)\n",
    "        titles.append(title)\n",
    "print (\"ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# usual thing, np arrays\n",
    "X = np.array(corpus)\n",
    "y = np.array(titles) \n",
    "\n",
    "# we define kmeans, with 10 clusters (you can change this number and see how the results change)\n",
    "# then we train it using only the documents\n",
    "kmeans = KMeans(n_clusters=10).fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# these are the labels we obtain\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see which docs are in which clusters, we need to loop over all labels\n",
    "\n",
    "# so the number of clusters\n",
    "for i in range(10):\n",
    "    # print the title of the document if the doc is in this cluster\n",
    "    print (\"this is cluster number\",i)\n",
    "    # then you loop over all titles\n",
    "    for k in range(len(titles)):\n",
    "        \n",
    "        # this is the title\n",
    "        title = titles[k]\n",
    "        \n",
    "        #this is its cluster label\n",
    "        label = kmeans.labels_[k]\n",
    "        \n",
    "        # does it belong to this cluster?\n",
    "        if i == label:\n",
    "            #if yes, then print it out!\n",
    "            print (title)\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's count which are the most popular words in the titles of each cluster\n",
    "from collections import Counter\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    # we create a list where we put the words from the titles\n",
    "    title_words = []\n",
    "    \n",
    "    print (\"this is cluster number\",i)\n",
    "    for k in range(len(titles)):\n",
    "        # we clean the title with our pipeline\n",
    "        title = nlp_pipeline(titles[k]).split(\" \")\n",
    "        label = kmeans.labels_[k]\n",
    "        if i == label:\n",
    "            # we put each word in the list\n",
    "            for word in title:\n",
    "                title_words.append(word)\n",
    "    \n",
    "    # then we count and print the 10 most common\n",
    "    most_common = Counter(title_words).most_common(10)\n",
    "    print (most_common)\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
