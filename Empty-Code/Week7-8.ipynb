{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word2vec tutorial: https://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "\n",
    "# download this file and add it in a folder named \"resources\"\n",
    "# this has to be in the same folder where you have \"Complete Code, \"Empty Code, etc\"\n",
    "# link: https://drive.google.com/drive/folders/1AGq5h67_m8D6JNj9va1opdF4oz8AQvwq?usp=sharing\n",
    "\n",
    "# this pre-trained model is organized like this: word = embeddings\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../resources/small-embeddings.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.27875   0.81053  -0.056025 -0.50963   0.38158   0.95391  -0.75521\n",
      " -0.08793  -0.28277   0.52398  -0.344     1.061    -0.45144  -0.20914\n",
      " -0.28174   0.13764   0.94775   0.036071 -1.0709    0.72811   0.80155\n",
      "  0.75824   0.53197  -0.72235  -0.21943  -0.80348   0.54585  -0.21601\n",
      " -0.24227  -0.70974  -0.42725  -0.085839  0.27275  -0.94201   0.6457\n",
      "  0.21807   0.22465   0.67688   0.74787  -0.25021   0.64739   0.73452\n",
      " -0.44746  -0.98194   0.39264   0.79881  -0.65376  -0.33434   0.026907\n",
      "  0.57293 ]\n"
     ]
    }
   ],
   "source": [
    "# is the corpus lowercased?\n",
    "print (model[\"sidney\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to see the embeddings of a word, you just do:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chicago', 0.9128427505493164),\n",
       " ('boston', 0.8992941975593567),\n",
       " ('angeles', 0.8561200499534607),\n",
       " ('d.c.', 0.8524180054664612),\n",
       " ('philadelphia', 0.8405068516731262),\n",
       " ('new', 0.8292818069458008),\n",
       " ('manhattan', 0.8184645175933838),\n",
       " ('washington', 0.8131545782089233),\n",
       " ('seattle', 0.7988473773002625),\n",
       " ('houston', 0.7823652625083923)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding similar words\n",
    "\n",
    "model.wv.most_similar(positive=[\"york\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9606045352860738\n"
     ]
    }
   ],
   "source": [
    "# get relatedness\n",
    "\n",
    "print (model.wv.similarity(\"obama\",\"clinton\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertisement By JACEY FORTINDEC. 31, 2017  In Sydney, rainbow fireworks sparkled off the Harbour Bridge in celebration of Australia’s recent legalization of gay marriage. (Sydney was among the first major cities to celebrate with fireworks at the stroke of midnight.) In Japan, people paraded in fox masks to attend the first prayer of the year at a Shinto shrine in Tokyo. In the Philippines, revelers gathered — phones in hand — at the Eastwood Mall in Manila to watch balloons and confetti rain down at midnight. Big pots of tea were prepared for New Year’s Eve celebrations in Beijing. The country will also celebrate the Lunar New Year, in February. It was raining in Singapore, but New Year’s Eve celebrants sheltered under umbrellas and raincoats as fireworks sparkled overhead. Tourists donned party hats to watch fireworks in front of the famous Petronas Twin Towers in Kuala Lumpur, Malaysia. Hundreds of couples got married at a mass wedding in Jakarta on New Year’s Eve. We’re interested in your feedback on this page. Tell us what you think. Go to Home Page »\n"
     ]
    }
   ],
   "source": [
    "# you can represent the meaning of an article, by the average of their embeddings\n",
    "# let's compute the embeddings for an article\n",
    "import codecs\n",
    "\n",
    "article = codecs.open(\"../datasets/CleanedArticles/15.txt\",\"r\",\"utf-8\")\n",
    "article = article.read()\n",
    "print (article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "# input should be a string - we need a simple pipeline for preparing the text that should be matched with the word embedding vocabulary\n",
    "def nlp_simple_pipeline(text):\n",
    "    \n",
    "    #it depends if the words have been lowercased or not\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = nltk.word_tokenize(text)\n",
    "        \n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    # as usual, be careful with this\n",
    "    text = [token for token in text if token not in stop_word_list]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why not stemming or lemmatization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['advertisement', 'jacey', 'fortindec', 'sydney', 'rainbow', 'fireworks', 'sparkled', 'harbour', 'bridge', 'celebration', 'australia', 'recent', 'legalization', 'gay', 'marriage', 'sydney', 'among', 'first', 'major', 'cities', 'celebrate', 'fireworks', 'stroke', 'midnight', 'japan', 'people', 'paraded', 'fox', 'masks', 'attend', 'first', 'prayer', 'year', 'shinto', 'shrine', 'tokyo', 'philippines', 'revelers', 'gathered', 'phones', 'hand', 'eastwood', 'mall', 'manila', 'watch', 'balloons', 'confetti', 'rain', 'midnight', 'big', 'pots', 'tea', 'prepared', 'new', 'year', 'eve', 'celebrations', 'beijing', 'country', 'also', 'celebrate', 'lunar', 'new', 'year', 'february', 'raining', 'singapore', 'new', 'year', 'eve', 'celebrants', 'sheltered', 'umbrellas', 'raincoats', 'fireworks', 'sparkled', 'overhead', 'tourists', 'donned', 'party', 'hats', 'watch', 'fireworks', 'front', 'famous', 'petronas', 'twin', 'towers', 'kuala', 'lumpur', 'malaysia', 'hundreds', 'couples', 'got', 'married', 'mass', 'wedding', 'jakarta', 'new', 'year', 'eve', 'interested', 'feedback', 'page', 'tell', 'us', 'think', 'go', 'home', 'page']\n"
     ]
    }
   ],
   "source": [
    "cleaned_article = nlp_simple_pipeline(article)\n",
    "print (cleaned_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advertisement\n",
      "ok, I have it!\n",
      "jacey\n",
      "ok, I have it!\n",
      "fortindec\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'fortindec' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-245b4f289d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_article\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0membed_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"ok, I have it!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'fortindec' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# for each word, load embeddings\n",
    "for word in cleaned_article:\n",
    "    print (word)\n",
    "    embed_word = model[word]\n",
    "    print (\"ok, I have it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# handling exceptions\n",
    "for word in cleaned_article:\n",
    "    try:\n",
    "        embed_word = model[word]\n",
    "    except KeyError:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embedd = []\n",
    "\n",
    "# for each word in the article, you take the embeddings\n",
    "for word in cleaned_article:\n",
    "    try:\n",
    "        embed_word = model[word]\n",
    "        article_embedd.append(embed_word)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "print (len(article_embedd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average vectors of all words\n",
    "doc_emb = [float(sum(col))/len(col) for col in zip(*article_embedd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# homework: try to write a function that does the same\n",
    "def create_doc_embedding(cleaned_article):\n",
    "    \n",
    "    # ....\n",
    "    \n",
    "    return doc_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
