{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# when should we use a list or a dictionary?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# so: how many tweets written by Donald Trump (and only by him!) contain the word \"great\"?\n",
    "\n",
    "import json\n",
    "\n",
    "with open('../datasets/trump.json') as f:\n",
    "    tweets = json.load(f)\n",
    "\n",
    "print (len(tweets))\n",
    "\n",
    "written_by_trump = 0\n",
    "with_great = 0\n",
    "for tweet in tweets:\n",
    "    if \"Android\" in tweet[\"source\"]:\n",
    "        text = tweet[\"text\"]\n",
    "        written_by_trump += 1\n",
    "        if \"great\" in text.lower():\n",
    "            with_great += 1\n",
    "        \n",
    "print (with_great, with_great/len(tweets),with_great/written_by_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we need a few libraries\n",
    "#os is needed to loop over files in a folder\n",
    "# codecs is for encoding a file\n",
    "#BeautifulSoup is needed for parsing the html of a scraped page\n",
    "\n",
    "import codecs,os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we prepare an empty list\n",
    "articles = []\n",
    "\n",
    "# we loop over a folder\n",
    "\n",
    "    # we check if the file is a txt file\n",
    "    \n",
    "        # we open and read the file\n",
    "        \n",
    "        # look at the original HTML - we remove the internet archive toolbar\n",
    "        html_page = str(doc).split(\"<!-- END WAYBACK TOOLBAR INSERT -->\")[1]\n",
    "            \n",
    "        # we parse the page\n",
    "        html_page = BeautifulSoup(html_page, \"lxml\")\n",
    "        \n",
    "        \n",
    "        # we open a new file in writing mode (using codecs) / we need to create the \"CleanedArticles\" folder if it's not there\n",
    "        output = codecs.open(\"../datasets/CleanedArticles/\"+filename,\"w\",\"utf-8\")\n",
    "        \n",
    "        # we define a new list, called text\n",
    "        text = []\n",
    "\n",
    "        # simply take all the paragraphs - we search for the elements \"p\"\n",
    "        for para in html_page.find_all('p'):\n",
    "            # we remove breaklines, tabs etc\n",
    "            para = para.text.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\r\",\" \")\n",
    "            text.append(para)\n",
    "        \n",
    "        # we put all paragraphs in a single text\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        # we write the text to the output file\n",
    "        output.write(text)\n",
    "        # we close the output file\n",
    "        output.close()\n",
    "        # we add the text to a list of articles\n",
    "        articles.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's take the first article of the list of articles\n",
    "article = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we need nltk - one of the most used text processing library in python\n",
    "\n",
    "import nltk # --> documentation: http://www.nltk.org/\n",
    "\n",
    "# you will also need this\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we start by dividing the text into sentences\n",
    "sentences =  # <-- documentation for this command: http://www.nltk.org/_modules/nltk/tokenize.html\n",
    "\n",
    "# let's print all the sentences, so we can exam the quality of the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's consider a single sentence - how do we do that? \n",
    "\n",
    "sentence = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's divide the sentence in tokens (aka single words)\n",
    "tokenized_sentence = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lower-casing the sentence\n",
    "without_capital_letters = \n",
    "# homework: write a for-loop for doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "\n",
    "# homework: download stopwords <- google it out\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# what is \"stop\" ?\n",
    "\n",
    "print (stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "without_stop_words = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# homework: how do we exclude punctuation? and numbers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
